"""
Test super simple para verificar conexiÃ³n con Ollama
"""

def test_ollama_direct():
    print("ğŸ§ª Probando conexiÃ³n directa con Ollama...")
    
    try:
        from langchain_ollama import OllamaLLM
        
        # Crear conexiÃ³n simple
        llm = OllamaLLM(
            model="nemotron-mini",
            base_url="http://localhost:11434",
            temperature=0.7,
        )
        
        print("âœ… Import de LangChain OK")
        
        # Prueba simple
        response = llm.invoke("Hello, how are you?")
        print(f"âœ… Respuesta de nemotron-mini: {response[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

if __name__ == "__main__":
    success = test_ollama_direct()
    if success:
        print("ğŸ‰ Â¡ConexiÃ³n bÃ¡sica funciona!")
    else:
        print("ğŸ’¥ Hay problemas de configuraciÃ³n")